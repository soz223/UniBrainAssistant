"""
UniBrain Streamlit App â€” concise, maintainable refactor
======================================================
(Outputs now appear inside collapsible **expanders** so the page stays tidy. Click to reveal/download.)

Author: Songlin Zhao (2025â€‘05â€‘01)
"""
from __future__ import annotations

# â”€â”€ Std / 3rdâ€‘party â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, uuid, time, warnings
from pathlib import Path

import numpy as np, torch, streamlit as st, SimpleITK as sitk, nibabel as nib
from PIL import Image
from skimage.transform import resize
import plotly.graph_objects as go

# â”€â”€ LangChain & tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain.agents import initialize_agent, AgentType
from langchain_core.tools import Tool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.schema import AIMessage, HumanMessage, SystemMessage

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
IMG_SIZE = 96
DEVICE   = torch.device("cpu")
ROOT     = Path(__file__).resolve().parent
ASSETS   = ROOT / "assets"
PDF_PATH = ROOT / "unibrain.pdf"
TXT_PATH = ROOT / "extra_knowledge.txt"
VDB_PATH = ROOT / "extra_knowledge.faiss"

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)


# ------------------------------------------------------------------#
# Sessionâ€‘state defaults                                            #
# ------------------------------------------------------------------#
DEFAULT_STEPS = [
    "Extraction", "Registration", "Segmentation",
    "Parcellation", "Network", "Classification",
]
if "selected_steps" not in st.session_state:
    st.session_state.selected_steps = DEFAULT_STEPS.copy()
if "pipeline_done" not in st.session_state:
    st.session_state.pipeline_done = False
if "messages" not in st.session_state:
    st.session_state.messages = []


# cross-version rerun helper
def _rerun():
    if hasattr(st, "rerun"):
        st.rerun()
    else:
        st.experimental_rerun()

# â”€â”€ utils_adj.py (æˆ–ç›´æ¥æ”¾åˆ°ç°æœ‰æ–‡ä»¶ä¸Šæ–¹) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch, networkx as nx
@st.cache_data
def load_adj(path: str) -> np.ndarray:
    """CPU-safeåŠ è½½ torch å­˜çš„é‚»æ¥çŸ©é˜µå¹¶è¿”å› numpy array"""
    return torch.load(path, map_location="cpu").cpu().numpy()

def adj_to_graph(arr: np.ndarray, thresh: float) -> nx.Graph:
    """æŠŠé‚»æ¥çŸ©é˜µè½¬æˆæ— å‘å›¾ï¼›è‡ªåŠ¨ squeeze åˆ° 2D"""
    arr2d = np.squeeze(arr)                # (1,N,N) -> (N,N)
    if arr2d.ndim != 2:
        raise ValueError(f"Adj must be 2-D, got shape {arr.shape}")
    mask = arr2d > thresh
    return nx.from_numpy_array(mask, create_using=nx.Graph)


def show_adj_heatmap(arr: np.ndarray):
    st.write("### Heatmap")
    arr2d = np.squeeze(arr)           # â† å»æ‰å¤šä½™ç»´åº¦
    fig, ax = plt.subplots(figsize=(5,5))
    im = ax.imshow(arr2d, aspect="equal")
    fig.colorbar(im, ax=ax, fraction=0.046)
    st.pyplot(fig, clear_figure=True)

import numpy as np
import networkx as nx

def adj_to_graph_random(arr: np.ndarray, keep_ratio: float, seed: int = 42) -> nx.Graph:
    """
    Randomly retain keep_ratio âˆˆ (0,1] of the 1-edges in a binary adjacency matrix.
    """
    A = np.squeeze(arr)                 # (1,N,N) â†’ (N,N)
    if A.ndim != 2:
        raise ValueError(f"Adj must be 2-D, got shape {arr.shape}")

    # --- pick edges from upper-triangular to avoid duplicates --------------
    rng   = np.random.default_rng(seed)
    idx_u = np.triu_indices_from(A, k=1)          # upper-tri indices
    ones  = np.flatnonzero(A[idx_u])              # positions of 1-edges
    if keep_ratio < 1.0:
        n_keep = max(1, int(len(ones) * keep_ratio))
        keep_idx = rng.choice(ones, size=n_keep, replace=False)
        mask_u   = np.zeros_like(idx_u[0], dtype=bool)
        mask_u[keep_idx] = True
    else:
        mask_u = np.zeros_like(idx_u[0], dtype=bool)
        mask_u[ones] = True

    # --- symmetric binary mask --------------------------------------------
    mask = np.zeros_like(A, dtype=bool)
    mask[idx_u] = mask_u
    mask = mask | mask.T               # make symmetric

    return nx.from_numpy_array(mask, create_using=nx.Graph)


def show_adj_graph(arr: np.ndarray, key_prefix: str):
    st.write("### Network graph ( X % of edges)")

    keep = st.slider(
        "Keep top-X % edges",
        5, 100, 20, 5,
        key=f"{key_prefix}_keep"
    ) / 100.0

    G = adj_to_graph_random(arr, keep_ratio=keep)

    if G.number_of_edges() == 0:
        st.info("No edges retained at this percentage.")
        return

    pos = nx.spring_layout(G, seed=42, k=1 / np.sqrt(max(G.number_of_nodes(), 1)))
    fig, ax = plt.subplots(figsize=(5, 5))
    nx.draw_networkx_nodes(G, pos, node_size=20, ax=ax)
    nx.draw_networkx_edges(G, pos, alpha=0.25, width=1, ax=ax)
    ax.set_title(f"{G.number_of_nodes()} nodes  |  {G.number_of_edges()} edges")
    ax.axis("off")
    st.pyplot(fig, clear_figure=True)





# â”€â”€ Model import (dummy fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from model import UniBrain  # type: ignore
    MODEL_OK = True
except ImportError:  # minimal stub
    class UniBrain(torch.nn.Module):
        def forward(self, *a, **k):
            b = a[1].shape[0]; shp = (b,1,IMG_SIZE,IMG_SIZE,IMG_SIZE)
            rand = lambda: torch.rand(shp, device=a[1].device)
            eye  = lambda: torch.eye(4, device=a[1].device).unsqueeze(0)
            return ([rand()]*3, [rand()]*3, [rand()]*3, [eye()]*2, [eye()]*2,
                    rand(), rand(), rand(), rand(), torch.rand(b,10,10), torch.rand(b,2))
    MODEL_OK = False

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cached loaders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
@st.cache_resource(show_spinner="â³ Loading templates â€¦")
def load_templates():
    def load(name: str):
        arr = np.load(ASSETS / name).astype(np.float32)
        return torch.from_numpy(arr)[None, None].to(DEVICE)
    try:
        return tuple(load(p) for p in ("tpl_img.npy","tpl_gm.npy","tpl_aal.npy"))
    except Exception as e:
        st.error(f"Template error: {e}"); return (None,None,None)

@st.cache_resource(show_spinner="â³ Loading UniBrain â€¦")
def load_model():
    if not MODEL_OK:
        st.warning("Using dummy model (import failed)"); return None
    try:
        m = UniBrain(img_size=IMG_SIZE, ext_stage=3, reg_stage=2, if_pred_aal=True).to(DEVICE)
        m.load_state_dict(torch.load(ASSETS/"unibrain.pth", map_location=DEVICE)); m.eval(); return m
    except Exception as e:
        st.error(f"Model load failed: {e}"); return None

REF_TPL, REF_GM, REF_AAL = load_templates(); MODEL = load_model()

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utility helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
NIIGZ = "application/gzip"

def nii_to_torch(p: Path):
    arr = sitk.GetArrayFromImage(sitk.ReadImage(str(p))).astype(np.float32)
    if arr.shape != (IMG_SIZE,)*3:
        arr = resize(arr,(IMG_SIZE,)*3,order=1,preserve_range=True,anti_aliasing=True)
    return torch.from_numpy(arr)[None,None].to(DEVICE)

def save_tensor(t: torch.Tensor, out: Path):
    t = torch.nan_to_num(t).cpu().squeeze().float()
    sitk.WriteImage(sitk.GetImageFromArray(t.numpy()), str(out))

def slice_png(t: torch.Tensor, out: Path):
    arr = t.squeeze().cpu().numpy()[t.shape[-1]//2]
    norm=((arr-arr.min())/(arr.ptp()+1e-6)*255).astype(np.uint8)
    Image.fromarray(norm).save(out)

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Knowledge RAG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
OPENAI_API_KEY = "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"

# â”€â”€ colour utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import matplotlib.pyplot as plt

def _make_cmap(n: int) -> np.ndarray:
    """
    ç”Ÿæˆ (n,3) float32 LUTï¼Œç´¢å¼• 0 ä¸ºé»‘ï¼Œå…¶ä½™æŒ‰ tab20/20b/20c è½®æµã€‚
    """
    base = (plt.get_cmap('tab20').colors +
            plt.get_cmap('tab20b').colors +
            plt.get_cmap('tab20c').colors) * 50        # ä¸Šåƒè‰²è¶³å¤Ÿ
    return np.vstack([[0, 0, 0], np.array(base[:n-1])]).astype(np.float32)

def roi_slice_to_rgb(label2d: np.ndarray) -> np.ndarray:
    """
    label2d: HÃ—W int32, ROI idï¼›èƒŒæ™¯=0
    è¿”å›  HÃ—WÃ—3 uint8 å½©è‰²å›¾ï¼ˆèƒŒæ™¯é»‘ï¼‰
    """
    max_id = int(label2d.max())
    lut = (_make_cmap(max_id + 1) * 255).astype(np.uint8)
    return lut[label2d]






@st.cache_resource(show_spinner="ğŸ” Building vector store â€¦")
def vector_store():
    if not OPENAI_API_KEY:
        st.error("Missing OpenAI key"); return None
    docs=[]
    if PDF_PATH.exists(): docs+=PyPDFLoader(str(PDF_PATH)).load()
    if TXT_PATH.exists(): docs+=TextLoader(str(TXT_PATH),encoding="utf-8").load()
    if not docs: st.error("No knowledge docs"); return None
    chunks=RecursiveCharacterTextSplitter(chunk_size=800,chunk_overlap=150).split_documents(docs)
    vdb=FAISS.from_documents(chunks,OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)); vdb.save_local(str(VDB_PATH)); return vdb
def ask_knowledge(q: str) -> str:
    vdb = vector_store()
    if vdb is None:
        return "Knowledge DB unavailable."

    # æ­£ç¡®è°ƒç”¨ as_retriever
    retriever = vdb.as_retriever(search_kwargs={"k": 4})
    docs = retriever.get_relevant_documents(q)
    ctx = "\n\n".join(d.page_content for d in docs)

    prompt = (
        "You are UniBrain-RAG assistant. Use the context only.\n\n"
        f"Context:\n{ctx}\n\n"
        f"Q: {q}"
    )
    return ChatOpenAI(
        openai_api_key=OPENAI_API_KEY,
        model_name="gpt-4o",
        temperature=0
    ).invoke(prompt).content



KNOWLEDGE_TOOL=Tool(name="ask_unibrain_paper",func=ask_knowledge,description="Answer UniBrain paper questions",return_direct=True)

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Inference pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
CARD=dict

from pathlib import Path
import time
import torch
import streamlit as st

# CARD alias already defined elsewhere
# MODEL, REF_TPL, REF_GM, REF_AAL, nii_to_torch, save_tensor all assumed imported

def run_pipeline(
    inp: Path,
    work: Path,
    steps: list[str] | None = None,    # <-- new param
) -> list[CARD]:
    if None in (MODEL, REF_TPL, REF_GM, REF_AAL):
        raise RuntimeError("Model/templates not ready")

    # default: run all steps
    all_steps = ["Extraction", "Registration", "Segmentation", "Parcellation", "Network", "Classification"]
    if steps is None:
        steps = all_steps

    mov = nii_to_torch(inp)
    st.info("Running UniBrain â€¦")
    t0 = time.time()
    with torch.no_grad():
        (
            striped,
            masks,
            warped,
            theta,
            theta_i,
            am_mov,
            am_ref2,
            aal_mov,
            aal_ref2,
            adj,
            logits,
        ) = MODEL(REF_TPL, mov, REF_GM, REF_AAL, if_train=False)
    st.success(f"Done in {time.time() - t0:.1f}s")

    nii_dir, pt_dir = (work / "nii", work / "pt")
    nii_dir.mkdir(parents=True, exist_ok=True)
    pt_dir.mkdir(exist_ok=True)

    cards: list[CARD] = []

    def nii_card(tensor, name, note=""):
        out = nii_dir / f"{name}.nii.gz"
        save_tensor(tensor, out)
        cards.append({
            "step": name.replace("_", " ").title(),
            "nifti_path": out,
            "explanation": note,
        })
        return out

    # Classification
    if "Classification" in steps:
        prob = torch.softmax(logits, 1)[0]
        cls, conf = int(prob.argmax()), float(prob.max())
        torch.save(logits.cpu(), pt_dir / "logits.pt")
        cards.append({
            "step": "Classification",
            "metrics": {"class": cls, "probability": conf},
            "explanation": f"Predicted **{cls}** (p={conf:.3f})",
            "file_path": pt_dir / "logits.pt",
        })

    # Segmentation / Parcellation
    if "Segmentation" in steps:
        nii_card(torch.argmax(am_mov, 1), "am_seg", "Anatomical mask")
    if "Parcellation" in steps:
        nii_card(torch.argmax(aal_mov, 1), "aal_seg", "AAL labels")

    # Registration details
    if "Registration" in steps:
        nii_card(am_ref2, "am_ref2mov", "Template ANATâ†’input")
        nii_card(aal_ref2, "aal_ref2mov", "Template AALâ†’input")
        for i, w in enumerate(warped, 1):
            nii_card(w, f"warped{i}")

    # Extraction masks
    if "Extraction" in steps:
        for i, (m, s) in enumerate(zip(masks, striped), 1):
            nii_card(m, f"mask{i}")
            nii_card(s, f"strip{i}")

    # Brain network (adjacency)
    if "Network" in steps:
        adj_path = pt_dir / "adj.pt"
        torch.save(adj, adj_path)
        cards.append({
            "step": "Network",
            "file_path": adj_path,
            "explanation": "Adjacency matrix",
        })

    return cards


# def run_pipeline(inp:Path,work:Path)->list[CARD]:
#     if None in (MODEL,REF_TPL,REF_GM,REF_AAL): raise RuntimeError("Model/templates not ready")
#     mov=nii_to_torch(inp)
#     st.info("Running UniBrain â€¦"); t0=time.time()
#     with torch.no_grad():
#         (striped,masks,warped,theta,theta_i,am_mov,am_ref2,aal_mov,aal_ref2,adj,logits)=MODEL(REF_TPL,mov,REF_GM,REF_AAL,if_train=False)
#     st.success(f"Done in {time.time()-t0:.1f}s")

#     nii_dir,pt_dir=(work/"nii",work/"pt"); nii_dir.mkdir(parents=True,exist_ok=True); pt_dir.mkdir(exist_ok=True)
#     cards: list[CARD]=[]
#     def nii_card(t,name,note=""):
#         out=nii_dir/f"{name}.nii.gz"; save_tensor(t,out)
#         cards.append({"step":name.replace("_"," ").title(),"nifti_path":out,"explanation":note}); return out

#     prob=torch.softmax(logits,1)[0]; cls,conf=int(prob.argmax()),float(prob.max())
#     torch.save(logits.cpu(),pt_dir/"logits.pt")
#     cards.append({"step":"Classification","metrics":{"class":cls,"probability":conf},"explanation":f"Predicted **{cls}** (p={conf:.3f})","file_path":pt_dir/"logits.pt"})

#     nii_card(torch.argmax(am_mov,1),"am_seg","Anatomical mask")
#     nii_card(torch.argmax(aal_mov,1),"aal_seg","AAL labels")
#     nii_card(am_ref2,"am_ref2mov","Template ANATâ†’input")
#     nii_card(aal_ref2,"aal_ref2mov","Template AALâ†’input")

#     for i,(m,s) in enumerate(zip(masks,striped),1):
#         nii_card(m,f"mask{i}"); nii_card(s,f"strip{i}")
#     for i,w in enumerate(warped,1):
#         nii_card(w,f"warped{i}")

#     for lbl,obj in {"theta":theta,"theta_inv":theta_i,"adj":adj}.items():
#         p=pt_dir/f"{lbl}.pt"; torch.save(obj,p); cards.append({"step":lbl,"file_path":p})

#     return cards

RUN_TOOL=Tool(name="run_unibrain_inference",description="Run/rerun UniBrain on uploaded file",func=lambda _: "â— No file" if "upload_path" not in st.session_state else _run_unibrain())

def _run_unibrain():
    if st.session_state.get("pipeline_done"): return "âœ… Already done"
    try:
        st.session_state.cards=run_pipeline(Path(st.session_state.upload_path),Path(st.session_state.upload_path).parent)
        st.session_state.pipeline_done=True; return "âœ… Inference complete"
    except Exception as e: return f"ğŸš¨ Failure: {e}"

# # â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agent init â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
# agent=None
# if OPENAI_API_KEY:
#     try:
#         agent=initialize_agent(agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,tools=[RUN_TOOL,KNOWLEDGE_TOOL],llm=ChatOpenAI(openai_api_key=OPENAI_API_KEY,model_name="gpt-4o",temperature=0),max_iterations=5,verbose=False)
#     except Exception as e: st.error(f"Agent init failed: {e}")
from pathlib import Path
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.memory import ConversationBufferWindowMemory

# â€”â€”â€” 1. è¯»å–å¤–éƒ¨ Prompt æ–‡ä»¶ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
PROMPT_PATH = ROOT / "prompts" / "unibrain_system_prompt.md"
SYSTEM_PROMPT = PROMPT_PATH.read_text(encoding="utf-8")

# â€”â€”â€” 2. æ„é€ å¸¦ system + human çš„ PromptTemplate â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(SYSTEM_PROMPT),
    HumanMessagePromptTemplate.from_template("{input}")
])

# â€”â€”â€” 3. åˆå§‹åŒ– Agent æ—¶æ³¨å…¥ prompt å’Œ memory â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
agent = None
if OPENAI_API_KEY:
    try:
        llm = ChatOpenAI(
            openai_api_key=OPENAI_API_KEY,
            model_name="gpt-4o-mini",
            temperature=0
        )
        memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True
        )
        agent = initialize_agent(
            agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
            tools=[RUN_TOOL, KNOWLEDGE_TOOL],
            llm=llm,
            prompt=prompt_template,           
            memory=memory,                    
            max_iterations=5,
            verbose=False,
            handle_parsing_errors=True
        )
        # st.write("ğŸ” Final agent.prompt:", agent.prompt or agent.agent.prompt_template)

    except Exception as e:
        st.error(f"Agent init failed: {e}")
        st.exception(e)





# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
@st.cache_data
def slice_vol(vol,z): sl=vol[:,:,z].astype(np.float32); rng=sl.ptp() or 1; return ((sl-sl.min())/rng*255).astype(np.uint8)

def viewer(nifti: Path, title: str, key: str, *, color: bool = False):
    vol = nib.load(str(nifti)).get_fdata()
    z_max = vol.shape[2] - 1
    z = st.slider(f"{title} â€“ slice", 0, z_max, z_max // 2, key=key)

    slice2d = vol[:, :, z]
    if color:  # AAL segmentation âœ å½©è‰²
        lab = slice2d.astype(np.int32)
        lab[lab < 1] = 0          # å¼ºåˆ¶èƒŒæ™¯=0
        st.image(
            roi_slice_to_rgb(lab),   # LUT[0] å·²æ˜¯çº¯é»‘
            caption=f"{title} (z={z})",
            use_container_width=True,
            clamp=True
        )

    else:
        st.image(slice_vol(vol, z), caption=f"{title} (z={z})",
                 use_container_width=True, clamp=True)


# â”€â”€ 1. é‡å†™ vol_figï¼šåªæ¥å—å‚æ•°ï¼Œä¸è°ƒ st â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def vol_fig(arr: np.ndarray, ds: int, colourscale: str) -> go.Figure:
    # é™é‡‡æ ·
    arr_ds = arr[::ds, ::ds, ::ds]
    # åæ ‡
    x, y, z = np.mgrid[
        :arr_ds.shape[0],
        :arr_ds.shape[1],
        :arr_ds.shape[2],
    ]
    # é˜ˆå€¼
    nonmin = arr_ds[arr_ds > arr_ds.min()]
    imin, imax = (np.percentile(nonmin, (5, 98))
                  if nonmin.size else (0, 1))

    fig = go.Figure(
        go.Volume(
            x=x.ravel(),
            y=y.ravel(),
            z=z.ravel(),
            value=arr_ds.ravel(),
            isomin=imin,
            isomax=imax,
            opacity=0.2,
            opacityscale=[
                [0, 0],
                [0.1, 0.05],
                [0.3, 0.1],
                [0.6, 0.3],
                [0.8, 0.5],
                [1, 0.7],
            ],
            surface_count=17,
            colorscale=colourscale,
        )
    )
    fig.update_layout(
        scene=dict(aspectmode="data"),
        margin=dict(l=0, r=0, t=30, b=0),
        height=500,
        title=dict(text="3-D Volume Preview", x=0.5),
    )
    return fig


# â”€â”€ 2. æ›´æ–°è¾“å…¥é¢„è§ˆï¼šç»™ input ä¹ŸåŠ å”¯ä¸€ key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def show_input_preview(in_path: Path) -> None:
    st.subheader("Input preview")
    viewer(in_path, "Input", "prev")  # ä¿æŒåŸæ¥çš„ 2D viewer

    # 3D æ¸²æŸ“å‚æ•°ï¼šå”¯ä¸€ key
    ds_key   = "quality_input"
    cmap_key = "cmap_input"
    ds_in = st.radio(
        "Quality (â†“ faster)",
        [4, 3, 2, 1],
        index=0,
        horizontal=True,
        key=ds_key,
    )
    cmap_in = st.selectbox(
        "Colormap",
        ["Greys", "Viridis", "Hot", "Rainbow", "Jet"],
        index=1,
        key=cmap_key,
    )

    arr = nib.load(str(in_path)).get_fdata()
    fig = vol_fig(arr, ds=ds_in, colourscale=cmap_in)
    st.plotly_chart(fig, use_container_width=True, key="vol_input")




# def vol_fig(arr:np.ndarray):
#     ds=st.radio("Quality (â†“ faster)",[4,3,2,1],index=1,horizontal=True)
#     arr=arr[::ds,::ds,::ds]
#     x,y,z=np.mgrid[:arr.shape[0],:arr.shape[1],:arr.shape[2]]
#     nonmin=arr[arr>arr.min()]; imin,imax=np.percentile(nonmin,(5,98)) if nonmin.size else (0,1)
#     return go.Figure(go.Volume(x=x.ravel(),y=y.ravel(),z=z.ravel(),value=arr.ravel(),isomin=imin,isomax=imax,opacity=0.2,opacityscale=[[0,0],[0.1,0.05],[0.3,0.1],[0.6,0.3],[0.8,0.5],[1,0.7]],surface_count=17,colorscale="Greys"))

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streamlit main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
st.set_page_config(page_title="UniBrain Agent",layout="centered", initial_sidebar_state="collapsed")

st.title("ğŸ§  UniBrain Assistant")
for k,v in {"messages":[],"pipeline_done":False}.items(): st.session_state.setdefault(k,v)

# â”€â”€ constants you already have â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NIIGZ = "application/gzip"

# ---------------------------------------------------------------------------#
# 0. å·¥å…·ï¼šåˆ¤å®šæ˜¯å¦ä¸º parcellationï¼ˆå†³å®šå½©è‰² / ç°é˜¶ï¼‰                        #
# ---------------------------------------------------------------------------#
def is_parcellation(step: str) -> bool:
    step = step.lower()
    return any(k in step for k in ("aal_seg", "aal_ref2mov"))

# ---------------------------------------------------------------------------#
# 1. æ–‡ä»¶ä¸Šä¼  & è½ç›˜                                                          #
# ---------------------------------------------------------------------------#
def handle_upload() -> Path | None:
    """Return local Path of uploaded NIfTI, or None if nothing yet."""
    upload = st.file_uploader("Upload NIfTI (.nii/.nii.gz)", ["nii", "nii.gz"])
    if not upload:
        # st.info("Upload a NIfTI file to begin.")
        return None

    # æ–°æ–‡ä»¶ â†’ ç”Ÿæˆ job ç›®å½•å¹¶ä¿å­˜
    if st.session_state.get("fname") != upload.name:
        job_id  = uuid.uuid4().hex[:8]
        workdir = Path("uploads") / job_id
        workdir.mkdir(parents=True, exist_ok=True)

        up_path = workdir / upload.name
        up_path.write_bytes(upload.getbuffer())

        st.session_state.update(
            fname          = upload.name,
            upload_path    = str(up_path),
            work_dir       = str(workdir),
            pipeline_done  = False,
            cards          = []
        )
        st.success(f"Saved â†’ {up_path}")

    return Path(st.session_state.upload_path)

# ---------------------------------------------------------------------------#
# 2. è¾“å…¥é¢„è§ˆï¼ˆä¸­é—´åˆ‡ç‰‡ + 3-D ä½“æ¸²æŸ“ï¼‰                                        #
# ---------------------------------------------------------------------------#
def show_input_preview(in_path: Path) -> None:
    st.subheader("Input preview")
    viewer(in_path, "Input", "prev")                      # 2-D slice viewer
    vol = nib.load(str(in_path)).get_fdata()
    # st.plotly_chart(vol_fig(vol), use_container_width=True)
    st.plotly_chart(vol_fig(vol, ds=2, colourscale="Greys"), use_container_width=True)



# ---------------------------------------------------------------------------#
# 3. â€œRun UniBrainâ€ æŒ‰é’®                                                      #
# ---------------------------------------------------------------------------#
def run_inference_button(in_path: Path) -> None:
    if st.button("Run UniBrain", disabled=st.session_state.pipeline_done, key="run_unibrain"):
        st.session_state.cards = run_pipeline(
            in_path, Path(st.session_state.work_dir)
        )
        st.session_state.pipeline_done = True
        st.rerun()


# 1) Let user select which steps to run
all_steps = ["Extraction", "Registration", "Segmentation",
             "Parcellation", "Network", "Classification"]

# sidebar default close

with st.sidebar.expander("âš™ï¸ Pipeline steps", expanded=False):
    st.write("Select which stages to run:")
    st.session_state.selected_steps = st.multiselect(
        "Stages",
        DEFAULT_STEPS,
        default=st.session_state.selected_steps,
        key="steps_select",
    )

selected_steps = st.session_state.selected_steps

# # 2) Run button
# if st.sidebar.button("Run UniBrain", disabled=st.session_state.pipeline_done):
#     # call with selected_steps
#     in_path = Path(st.session_state.upload_path)
#     work_dir = in_path.parent
#     st.session_state.cards = run_pipeline(in_path, work_dir, selected_steps)
#     st.session_state.pipeline_done = True
#     st.rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sidebar run button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.sidebar.button(
        "Run UniBrain",
        disabled=st.session_state.pipeline_done,
        key="sidebar_run"):
    if "upload_path" not in st.session_state:
        st.error("â—Â Please upload a file first.")
    else:
        in_path  = Path(st.session_state.upload_path)
        work_dir = in_path.parent

        st.session_state.cards = run_pipeline(
            in_path,
            work_dir,
            st.session_state.selected_steps        # â† pass steps
        )
        st.session_state.pipeline_done = True

        # 1ï¸âƒ£ append friendly summary *before* rerun
        summary = "âœ… Finished running: " + ", ".join(st.session_state.selected_steps)
        st.session_state.messages.append({"role": "assistant", "content": summary})

        _rerun()   # â† do this last



# ---------------------------------------------------------------------------#
# 4. ç»“æœå±•ç¤ºï¼šæŠ˜å å¡ç‰‡ + ä¸‹è½½                                                #
# ---------------------------------------------------------------------------#
def show_outputs() -> None:
    if not st.session_state.get("pipeline_done"):
        return

    st.header("Inference Outputs")
    for i, card in enumerate(st.session_state.cards):
        with st.expander(card["step"], expanded=False):
            st.write(card.get("explanation", ""))

            # â”€â”€ â‘  åˆ†ç±»æŒ‡æ ‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if m := card.get("metrics"):
                c1, c2 = st.columns(2)
                c1.metric("Class",       m["class"])
                c2.metric("Confidence", f"{m['probability']:.3f}")

            # â”€â”€ â‘¡ å¦‚æœæœ‰ NIfTIï¼Œå°±ç»™ä¸ªæ˜¾ç¤ºæ¨¡å¼åˆ‡æ¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            if (p := card.get("nifti_path")) and Path(p).exists():
                arr = nib.load(str(p)).get_fdata()

                view_mode = st.radio(
                    "Display mode",
                    ["2D slice", "3D volume"],
                    index=0,
                    key=f"view_{i}",
                )

                if view_mode == "2D slice":
                    viewer(
                        Path(p),
                        card["step"],
                        f"out_{i}",
                        color=is_parcellation(card["step"]),
                    )
                else:
                    # å…ˆé€‰è´¨é‡ä¸‹é‡‡æ ·
                    ds_i = st.radio(
                        "Quality (â†“ faster)",
                        [4, 3, 2, 1],
                        index=1,
                        horizontal=True,
                        key=f"quality_{i}",
                    )
                    # å†é€‰è°ƒè‰²æ¿
                    cmap_i = st.selectbox(
                        "Colormap",
                        ["Greys", "Viridis", "Hot", "Rainbow", "Jet"],
                        index=1,
                        key=f"cmap_{i}",
                    )
                    fig = vol_fig(arr, ds=ds_i, colourscale=cmap_i)
                    st.plotly_chart(fig, use_container_width=True, key=f"vol_{i}")

                # ä¸‹è½½æŒ‰é’®
                st.download_button(
                    "Download NIfTI",
                    Path(p).read_bytes(),
                    file_name=Path(p).name,
                    mime=NIIGZ,
                    key=f"dl_{i}"
                )
            # â”€â”€ â‘¡-b  è‹¥æ˜¯ adj.pt å°±æ˜¾ç¤º Heatmap / Graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            elif (fp := card.get("file_path")) and "adj" in fp.stem:
                arr = load_adj(str(fp))
                view_mode = st.radio(
                    "Adjacency view",
                    ["Heatmap", "Graph"],
                    index=0,
                    key=f"adj_view_{i}"
                )
                if view_mode == "Heatmap":
                    show_adj_heatmap(arr)
                else:
                    show_adj_graph(arr, key_prefix=f"adj_{i}")

                # ä¸‹è½½æŒ‰é’®
                st.download_button(
                    "Download adj.pt",
                    Path(fp).read_bytes(),
                    file_name=Path(fp).name,
                    mime="application/octet-stream",
                    key=f"dl_adj_{i}"
                )

            # â”€â”€ â‘¢ å…¶å®ƒäºŒè¿›åˆ¶è¾“å‡º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (fp := card.get("file_path")) and Path(fp).exists():
                st.download_button(
                    "Download data",
                    Path(fp).read_bytes(),
                    file_name=Path(fp).name,
                    mime="application/octet-stream",
                    key=f"dlf_{i}"
                )

            

CMD_SYS_PROMPT = """
You are a controller for the UniBrain Streamlit app. 
Your ONLY job is to examine the user's sentence and decide
â€¢ whether it requests skipping, enabling, or resetting pipeline steps.
Valid step names: Extraction, Registration, Segmentation, Parcellation, Network, Classification.

Treat these patterns all as â€œskip <Step>â€ commands:
  - â€œskip segmentationâ€
  - â€œwithout segmentationâ€
  - â€œno segmentationâ€
  - â€œpreprocessing without segmentationâ€
  - â€œdo the preprocessing without segmentationâ€
  - â€œrun pipeline without segmentationâ€

OUTPUT RULES:
1. If the user wants to skip a step, output EXACT JSON: {"action":"skip","step":"<Step>"}
2. If the user wants to enable a step, output EXACT JSON: {"action":"enable","step":"<Step>"}
3. If the user wants to reset everything, output EXACT JSON: {"action":"reset"}
4. Otherwise output the literal word: none

Do NOT wrap JSON in markdown; no extra text.
""".strip()



from openai import OpenAI
import re, json, streamlit as st
from typing import Optional, Tuple

client = OpenAI(api_key=OPENAI_API_KEY)


DEFAULT_STEPS = [
    "Extraction","Registration","Segmentation",
    "Parcellation","Network","Classification"
]

def regex_detect(cmd: str) -> Optional[Tuple[str,str]]:
    cmd = cmd.lower().strip()
    # reset
    if "reset steps" in cmd or "reset pipeline" in cmd:
        return ("reset","")
    # skip patterns
    m = re.match(r"(skip|without|no)\s+(\w+)", cmd)
    if m:
        action, step = m.groups()
        if action in ("without","no"):
            action = "skip"
        return (action, step.capitalize())
    # enable
    m = re.match(r"(enable|turn on)\s+(\w+)", cmd)
    if m:
        action, step = m.groups()
        return ("enable", step.capitalize())
    return None

def llm_detect_command(text: str) -> Optional[Tuple[str,str]]:
    try:
        resp = client.chat.completions.create(
            model       = "gpt-3.5-turbo-0125",
            messages    = [
                {"role":"system","content":CMD_SYS_PROMPT},
                {"role":"user",  "content":text},
            ],
            max_tokens  = 20,
            temperature = 0,
        )
        out = resp.choices[0].message.content.strip()
        if out == "none":
            return None
        data = json.loads(out)
        action = data.get("action")
        if action == "reset":
            return ("reset","")
        if action in ("skip","enable") and data.get("step"):
            return (action, data["step"].capitalize())
    except Exception as e:
        st.warning(f"Cmd-parser LLM error: {e}")
    return None

def detect_command(text: str) -> Optional[Tuple[str,str]]:
    # 1) try regex
    cmd = regex_detect(text)
    if cmd:
        return cmd
    # 2) fallback to LLM
    return llm_detect_command(text)




# ---------------------------------------------------------------------------#
# ğŸƒ ä¸»æµç¨‹ï¼šä¸€æ­¥ä¸€å‡½æ•°                                                       #
# ---------------------------------------------------------------------------#
in_path = handle_upload()
if in_path:
    show_input_preview(in_path)
    # run_inference_button(in_path)
    show_outputs()











# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Chat area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
st.divider()
st.subheader("Chat with UniBrain Assistant")

# 0) Render history
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# 1) Show single input box only when no pending reply
waiting_reply = (
    st.session_state.messages
    and st.session_state.messages[-1]["role"] == "user"
)
if not waiting_reply and agent:
    user_text = st.chat_input(
        "Ask about UniBrain â€¦",
        key="main_chat_input"
    )
else:
    user_text = None

# 2) Detect & handle commands vs normal chat
if user_text:
    cmd = detect_command(user_text)
    # st.write(f"ğŸ” Debug: detect_command -> {cmd}")
    if cmd:
        action, step = cmd
        # â”€â”€ Update the selected_steps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if action == "reset":
            st.session_state.selected_steps = DEFAULT_STEPS.copy()
            ack = "ğŸ”„ Steps reset to all enabled."
        elif action == "skip":
            st.session_state.selected_steps = [
                s for s in st.session_state.selected_steps if s != step
            ]
            ack = f"â­ï¸ **{step}** skipped."
        else:  # enable
            if step not in st.session_state.selected_steps:
                st.session_state.selected_steps.append(step)
            ack = f"âœ… **{step}** enabled."

        # â”€â”€ Immediately re-run the pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.session_state.pipeline_done = False
        # you will need `Path` imported at top: from pathlib import Path
        in_path  = Path(st.session_state.upload_path)
        work_dir = in_path.parent
        st.session_state.cards = run_pipeline(
            in_path, work_dir, st.session_state.selected_steps
        )
        st.session_state.pipeline_done = True

        # â”€â”€ Echo back to the chat UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.session_state.messages.append({"role":"user",      "content":user_text})
        st.session_state.messages.append({"role":"assistant", "content":ack})


        _rerun()    
    else:
        # normal chat â†’ queue LLM, *do not* add summary yet
        st.session_state.messages.append({"role": "user", "content": user_text})
        _rerun()


# 3) If waiting_reply, call agent exactly once
if waiting_reply and agent:
    question = st.session_state.messages[-1]["content"]
    ctx      = f"File: {st.session_state.upload_path}" if "upload_path" in st.session_state else "No file."

    # build history for agent
    hist = []
    for m in st.session_state.messages[:-1]:
        if m["role"] == "user":
            hist.append(HumanMessage(content=m["content"]))
        else:
            hist.append(AIMessage   (content=m["content"]))

    full_history = [SystemMessage(content=SYSTEM_PROMPT)] + hist
    full_history.append(HumanMessage(content=f"{question}\n\nContext: {ctx}"))

    with st.chat_message("assistant"), st.spinner("Thinkingâ€¦"):
        try:
            res = agent.invoke({"input": full_history})
            answer = res.get("output", str(res))
        except Exception as e:
            answer = f"Agent error: {e}"

        st.session_state.messages.append({"role":"assistant","content":answer})
        _rerun()

elif not agent:
    st.info("Agent disabled (no API key)")
