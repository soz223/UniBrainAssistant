# ğŸ§ â€¯UniBrainâ€‘Assistant


**UniBrainâ€‘Assistant** is an open-source, browser-based platform that integrates end-to-end deep learning into a fully conversational workflow for structural brain MRI analysis.

### :rocket: Try Me - One Click, Done Quick!

You can drop in a NIfTI file, watch every preprocessing step unfold in real time, explore the resulting connectome interactively, and ask questions in plain English or any natural languages â€” all without leaving your web browser.

It pairs Streamlitâ€™s reactive UI with LangChainâ€™s toolâ€‘calling so you can **see**, **tweak**, and **interrogate** each stage of the pipeline:

* skullâ€‘stripping â†’ affine registration â†’ tissue segmentation â†’ AAL parcellation â†’ graph construction â†’ disease classification
* fully **interactive**: 2D slice viewer, 3D Plotly volume, heatâ€‘map / graph visualisations, oneâ€‘click downloads
* **pipeline orchestration by naturalâ€‘language** â€“ e.g. `run the pipeline without segmentation`, `enable network`
* **RAGâ€‘powered Qâ€¯&â€¯A** over both your outputs **and** the UniBrain paper itself

### â³ We are working hard to enhance the tool, and a new version will be released soon.

---

## âœ¨Â Key Features

| UIâ€¯/â€¯UX                                     | Details                                                                                                     |
| ------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **Dragâ€‘&â€‘drop NIfTI** (`.nii`â€¯/â€¯`.nii.gz`)  | Files stored under `uploads/<8â€‘charâ€‘id>/` for easy cleanup                                                  |
| **Smart reruns**                            | Upload survives every Streamlit rerun â€“ viewers and cards never disappear                                   |
| **Collapsible output cards**                | Keep the page tidy; expand only what you need                                                               |
| **2D/3D switch**                            | Fast slice slider **or** Plotly volume (quality slider + colourâ€‘map)                                        |
| **Adjacency exploration**                   | Toggle *heatâ€‘map* or *interactive network graph* (edgeâ€‘density slider)                                      |
| **Download buttons everywhere**             | NIfTI (`.nii.gz`) or raw PyTorch (`.pt`)                                                                    |
| **Sidebar â€œâš™ï¸â€¯Pipeline stepsâ€** selector    | Tick/untick *Extraction, Registration, â€¦* before running                                                    |
| **Naturalâ€‘language controller**             | `skip segmentation`, `reset steps`, `enable classification`, â€¦                                              |
| **Chat assistant** (GPTâ€‘4oâ€‘mini by default) | â€¢ answers neuroscience questions via the RAG tool<br>â€¢ can call `run_unibrain_inference` tool automatically |

---

## ğŸ—ï¸Â Project Layout

```text
â”œâ”€ app.py                  â† this Streamlit app (singleâ€‘file, selfâ€‘contained)
â”œâ”€ assets/
â”‚  â”œâ”€ tpl_img.npy          â† template volume
â”‚  â”œâ”€ tpl_gm.npy           â† template GM mask
â”‚  â””â”€ tpl_aal.npy          â† template AAL labels
â”œâ”€ model.py                â† UniBrain network (or dropâ€‘in)
â”œâ”€ prompts/
â”‚  â””â”€ unibrain_system_prompt.md
â”œâ”€ unibrain.pdf            â† paper for RAG
â””â”€ extra_knowledge.txt     â† any supplementary text you like
```

### ğŸ”¬Â Method Structure

<p align="center">
  <img src="./figures/structure.png" alt="Endâ€‘toâ€‘end processing pipeline" width="100%"/>
</p>

> **No UniBrain weights?**
> If `assets/unibrain.pth` is missing the app loads a **dummy stub** so you can
> still explore the UI.

---

## ğŸš€Â Quick Start

```bash
git clone https://github.com/<yourâ€‘handle>/unibrain-assistant.git
cd unibrain-assistant
python -m venv .venv && source .venv/bin/activate      # optional
pip install -r requirements.txt
export OPENAI_API_KEY="sk-..."                         # GPTâ€‘4oâ€‘mini / 3.5â€‘turbo
streamlit run app.py
```

Open [http://localhost:8501](http://localhost:8501) â†’ upload a NIfTI â†’ pick steps â†’ **Run**.
Then talk to your data:

```
â¯ without segmentation                 # skips Segmentation, reâ€‘runs
â¯ what does a high dice score mean?
â¯ show me only the brain network stage
```

---

## ğŸ”‘Â Environment Variables

| Var              | Purpose                                        |
| ---------------- | ---------------------------------------------- |
| `OPENAI_API_KEY` | Required for chat, commandâ€‘parser LLM, and RAG |
| `IMG_SIZE`       | (optional) override default 96Â³ voxel size     |

---

## ğŸ“¦Â Core Dependencies

* `streamlit â‰¥1.32`
* `torch`, `numpy`, `nibabel`, `SimpleITK`, `plotly`, `networkx`
* `langchain`, `langchainâ€‘openai`, `faissâ€‘cpu`
* `openai` (â‰¥1.0 python SDK)

See `requirements.txt` for exact versions.

---

## ğŸ¤–Â Command Grammar (for reference)

| Intent          | Examples (caseâ€‘insensitive)                               |
| --------------- | --------------------------------------------------------- |
| **Skip step**   | `skip segmentation`, `no network`, `without registration` |
| **Enable step** | `enable classification`, `turn on parcellation`           |
| **Reset**       | `reset steps`, `reset pipeline`                           |
| Anything else   | routed to the regular chat assistant                      |

Internally the message goes through:

1. **Regex fastâ€‘path**
2. If unresolved â†’ **GPTâ€‘4oâ€‘mini** prompt (`CMD_SYS_PROMPT`) â†’ JSON response.

---


---

## ğŸ–¼ï¸Â Demo

<p align="center">
  <img src="./figures/demo1.png" alt="Upload & preprocessing" width="100%"/>
  <img src="./figures/demo2.png" alt="Interactive slice viewer" width="100%"/>
  <img src="./figures/demo3.png" alt="3â€‘D volumetric viewer" width="100%"/>
</p>
<p align="center">
  <img src="./figures/demo4.png" alt="Graph visualisation" width="100%"/>
  <img src="./figures/demo5.png" alt="Chatâ€‘driven control" width="100%"/>
</p>


## ğŸ“Â Contributing

PRs are welcome! Interesting directions:

* plugâ€‘in **nonâ€‘rigid** registration backâ€‘ends
* support **multiâ€‘modal** inputs (fMRI + DTI)
* switch 3â€‘D viewer to `vtk.js` for volume clipping planes
* add **batch mode** & progress bars



---

## ğŸ“šÂ Citation

Please cite the following work if UniBrainâ€‘Assistant contributes to your research:

```bibtex
@article{su2025end,
  title={End-to-End Deep Learning for Structural Brain Imaging: A Unified Framework},
  author={Su, Yao and Han, Keqi and Zeng, Mingjie and Sun, Lichao and Zhan, Liang and Yang, Carl and He, Lifang and Kong, Xiangnan},
  journal={arXiv preprint arXiv:2502.18523},
  year={2025}
}
```


---

## ğŸ“„Â License

MIT â€“ do whatever you want, but please cite the UniBrain paper if you use the
model for research.

